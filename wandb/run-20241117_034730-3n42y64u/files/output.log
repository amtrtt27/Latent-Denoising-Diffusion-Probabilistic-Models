11/17/2024 03:47:30 - INFO - __main__ - ***** Training arguments *****
11/17/2024 03:47:30 - INFO - __main__ - Namespace(config='configs/ddpm.yaml', data_dir='./data/imagenet100_128x128', image_size=128, batch_size=16, num_workers=4, num_classes=100, run_name='exp-10-ddpm', output_dir='experiments', num_epochs=10, learning_rate=0.001, weight_decay=0.0001, grad_clip=1.0, seed=42, mixed_precision='none', num_train_timesteps=1000, num_inference_steps=50, beta_start=0.0001, beta_end=0.02, beta_schedule='linear', variance_type='fixed_small', prediction_type='epsilon', clip_sample=True, clip_sample_range=1.0, unet_in_size=128, unet_in_ch=3, unet_ch=128, unet_ch_mult=[1, 2, 2, 4], unet_attn=[2, 3], unet_num_res_blocks=2, unet_dropout=0.0, latent_ddpm=False, use_cfg=False, cfg_guidance_scale=2.0, use_ddim=False, ckpt=None, predictor_type='epsilon', distributed=False, world_size=1, rank=0, local_rank=0, device='cuda', total_batch_size=16, max_train_steps=81250)
11/17/2024 03:47:30 - INFO - __main__ - ***** Running training *****
11/17/2024 03:47:30 - INFO - __main__ -   Num examples = 130000
11/17/2024 03:47:30 - INFO - __main__ -   Num Epochs = 10
11/17/2024 03:47:30 - INFO - __main__ -   Instantaneous batch size per device = 16
11/17/2024 03:47:30 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16
11/17/2024 03:47:30 - INFO - __main__ -   Total optimization steps per epoch 8125
11/17/2024 03:47:30 - INFO - __main__ -   Total optimization steps = 81250
/home/ubuntu/idl/Latent-Denoising-Diffusion-Probabilistic-Models/train.py:271: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
  0%|                                                                                                                                                                                 | 0/8125 [00:00<?, ?it/s]11/17/2024 03:47:30 - INFO - __main__ - Epoch 1/10
/home/ubuntu/idl/Latent-Denoising-Diffusion-Probabilistic-Models/train.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  0%|                                                                                                                                                                       | 1/8125 [00:01<3:03:47,  1.36s/it]11/17/2024 03:47:31 - INFO - __main__ - Epoch 1/10, Step 0/8125, Loss 0.9971854090690613 (0.9971854090690613)
  1%|█▋                                                                                                                                                                      | 82/8125 [00:36<58:24,  2.29it/s]Traceback (most recent call last):
  File "/home/ubuntu/idl/Latent-Denoising-Diffusion-Probabilistic-Models/train.py", line 407, in <module>
    main()
  File "/home/ubuntu/idl/Latent-Denoising-Diffusion-Probabilistic-Models/train.py", line 355, in main
    scaler.step(optimizer) # This is a replacement for optimizer.step()
    ^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 457, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 352, in _maybe_opt_step
    retval = optimizer.step(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/torch/optim/adamw.py", line 130, in _init_group
    state = self.state[p]
            ~~~~~~~~~~^^^
  File "/opt/conda/lib/python3.12/site-packages/torch/_tensor.py", line 1121, in __hash__
    def __hash__(self):

KeyboardInterrupt
