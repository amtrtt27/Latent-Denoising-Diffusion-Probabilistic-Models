11/17/2024 06:03:42 - INFO - __main__ - ***** Training arguments *****
11/17/2024 06:03:42 - INFO - __main__ - Namespace(config='configs/ddpm.yaml', test_imple=False, data_dir='./data/imagenet100_128x128', image_size=128, batch_size=30, num_workers=4, num_classes=None, run_name='exp-24-ddpm', output_dir='experiments', num_epochs=10, learning_rate=0.001, weight_decay=0.0001, grad_clip=1.0, seed=42, mixed_precision='none', num_train_timesteps=1000, num_inference_steps=50, beta_start=0.0001, beta_end=0.02, beta_schedule='linear', variance_type='fixed_small', prediction_type='epsilon', clip_sample=True, clip_sample_range=1.0, unet_in_size=128, unet_in_ch=3, unet_ch=128, unet_ch_mult=[1, 2, 2, 4], unet_attn=[2, 3], unet_num_res_blocks=2, unet_dropout=0.0, latent_ddpm=False, use_cfg=False, cfg_guidance_scale=None, use_ddim=False, ckpt=None, predictor_type='epsilon', distributed=False, world_size=1, rank=0, local_rank=0, device='cuda', total_batch_size=30, max_train_steps=43340)
11/17/2024 06:03:42 - INFO - __main__ - ***** Running training *****
11/17/2024 06:03:42 - INFO - __main__ -   Num examples = 130000
11/17/2024 06:03:42 - INFO - __main__ -   Num Epochs = 10
11/17/2024 06:03:42 - INFO - __main__ -   Instantaneous batch size per device = 30
11/17/2024 06:03:42 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 30
11/17/2024 06:03:42 - INFO - __main__ -   Total optimization steps per epoch 4334
11/17/2024 06:03:42 - INFO - __main__ -   Total optimization steps = 43340
  0%|                                                                                                                                                                                 | 0/4334 [00:00<?, ?it/s]11/17/2024 06:03:42 - INFO - __main__ - Epoch 1/10
  0%|                                                                                                                                                 | 1/4334 [00:01<2:05:40,  1.74s/it, loss=0.9980 (0.9980)]11/17/2024 06:03:44 - INFO - __main__ - Epoch 1/10, Step 0/4334, Loss 0.9979665279388428 (0.9979665279388428)
  0%|▎                                                                                                                                                 | 11/4334 [00:09<57:03,  1.26it/s, loss=0.9128 (0.9758)]11/17/2024 06:03:52 - INFO - __main__ - Epoch 1/10, Step 10/4334, Loss 0.9128249287605286 (0.9758374203335155)
  0%|▋                                                                                                                                                 | 21/4334 [00:17<56:28,  1.27it/s, loss=0.7014 (0.8987)]11/17/2024 06:03:59 - INFO - __main__ - Epoch 1/10, Step 20/4334, Loss 0.7013741731643677 (0.8986745249657404)
  1%|█                                                                                                                                                 | 30/4334 [00:24<56:12,  1.28it/s, loss=0.5332 (0.7984)]Exception in thread Thread-4 (_pin_memory_loop):
Traceback (most recent call last):
  File "/opt/conda/lib/python3.12/threading.py", line 1075, in _bootstrap_inner
    self.run()
  File "/opt/conda/lib/python3.12/threading.py", line 1012, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py", line 59, in _pin_memory_loop
    do_one_step()
  File "/opt/conda/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py", line 35, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/torch/multiprocessing/reductions.py", line 541, in rebuild_storage_fd
    fd = df.detach()
         ^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/multiprocessing/connection.py", line 519, in Client
    c = SocketClient(address)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/multiprocessing/connection.py", line 647, in SocketClient
    s.connect(address)
FileNotFoundError: [Errno 2] No such file or directory
Traceback (most recent call last):
  File "/home/ubuntu/idl/Latent-Denoising-Diffusion-Probabilistic-Models/train.py", line 414, in <module>
    main()
  File "/home/ubuntu/idl/Latent-Denoising-Diffusion-Probabilistic-Models/train.py", line 339, in main
    model_pred = unet(noisy_images, timesteps, class_emb)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/idl/Latent-Denoising-Diffusion-Probabilistic-Models/models/unet.py", line 92, in forward
    h = torch.cat([h, hs.pop()], dim=1)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
